### Gemma 3 Projector + LoRA Training Config for LLaMA-Factory ###
### Use this if you also want to fine-tune LLM with LoRA while training projector ###

### Model ###
model_name_or_path: /workspace/leo/mllm/models/gemma3-4b-it
trust_remote_code: true

### Method ###
stage: sft
do_train: true
finetuning_type: lora

### LoRA ###
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target: all               # Apply LoRA to all linear layers

### Freeze settings ###
freeze_vision_tower: true              # Freeze vision encoder
freeze_multi_modal_projector: false    # Train projector
# With LoRA, LLM weights are frozen but LoRA adapters are trained

### Dataset ###
dataset_dir: /workspace/leo/mllm/data
dataset: gemma3_vlm_train
template: gemma3
cutoff_len: 2048
preprocessing_num_workers: 4
dataloader_num_workers: 4

### Output ###
output_dir: /workspace/leo/mllm/outputs/gemma3_projector_lora
logging_steps: 10
save_steps: 500
save_total_limit: 3
overwrite_output_dir: true

### Training ###
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_grad_norm: 1.0

### Mixed Precision ###
bf16: true
ddp_timeout: 180000000

### Other ###
seed: 42
report_to: none

