### Gemma 3 Projector-Only Training Config for LLaMA-Factory ###

### Model ###
model_name_or_path: /workspace/leo/mllm/models/gemma3-4b-it
trust_remote_code: true

### Method ###
stage: sft
do_train: true
finetuning_type: full

### Freeze settings (PROJECTOR-ONLY TRAINING) ###
freeze_vision_tower: true              # Freeze vision encoder (SigLIP)
freeze_language_model: true            # Freeze text model (Gemma3)
freeze_multi_modal_projector: false    # Train projector

### Dataset ###
dataset_dir: /workspace/leo/mllm/data
dataset: gemma3_vlm_train
template: gemma3
cutoff_len: 2048
preprocessing_num_workers: 4
dataloader_num_workers: 4

### Output ###
output_dir: /workspace/leo/mllm/outputs/gemma3_projector_lf
logging_steps: 10
save_steps: 500
save_total_limit: 3
overwrite_output_dir: true

### Training ###
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
max_grad_norm: 1.0

### Mixed Precision ###
bf16: true
ddp_timeout: 180000000

### Evaluation ###
# val_size: 0.1
# per_device_eval_batch_size: 2
# eval_strategy: steps
# eval_steps: 500

### Other ###
seed: 42
report_to: none
save_safetensors: false